---
title: "analyses"
author: "Matt Blanchard"
date: "28/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r load, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(corrr)
library(knitr)
library(corrplot)
library(psych)
library(car) # load package to diagnose multicollinearity
library(GGally)
# library(olsrr) # print semi-part correlations
library(mctest)

# read sim variables
vars <- read_csv(here("data/200309_comms_efa_vars.csv")) %>% 
      filter(!team %in% c("17080712_1", "17080810_1"))  # remove outliers: collisions("17080810_1"), distance("17081510_2")

# read raw comms data
raw <- read_csv(here("data/200221_comms_raw.csv")) %>% 
  filter(team %in% vars$team) # remove outlier teams

# select which communication variables to analyse
comms_fac <- names(vars %>% select(inconsistent_codriver, terrible_codriver, helpful_exchange))

# select which sim-metrics to analyse
sim_metrics <- names(vars %>% select(collisions_overall, speed_overall, distance_overall))

```


# Descriptives statistics, distributions, correlations, and reliability estimates
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# utility functions
# function for descriptives
descriptives <- function(data) {
  x <- data %>% 
    gather(var, val) %>% 
    group_by(var) %>% 
    summarise(mean = round(mean(val, na.rm = T),2),
              sd = round(sd(val, na.rm = T),2),
              min = round(min(val, na.rm = T),2),
              max = round(max(val, na.rm = T),2))
  
}

# function for distributions
distributions <- function(data) {
  data %>%  
    gather(var, val) %>% 
    ggplot(aes(x = val)) +
    geom_histogram() +
    facet_wrap(~var, scales = "free") +
    theme_minimal()
}

# function for correlation matrices with sig. stars
source(here("R/corr_matrix_sig.R"))

```

## Driving simulation metrics
## Overall
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# Overall
# select variables
x <- vars %>% 
  select(sim_metrics, distance_overall_deviation)

# descriptives
descriptives(x)

# distributions
distributions(x)

# correlations
kable(star_matrix(x))

# reliability estimates
name <- c("collisions", "speed", "distance", "distance_deviation")

map(name, function(i) {
  x <- vars %>%
    ungroup() %>% 
    select(team, collisions_1:collisions_5, speed_1:speed_5, 
           distance_1:distance_5,
           distance_1_deviation:distance_5_deviation) %>% 
    gather(var, val, -team) %>% 
    mutate(lap = str_extract(var, "_[1-5]"),
           lap = as.numeric(str_remove(lap, "_")),
           var = str_remove(var, "_[1-5]")) %>% 
    filter(var == i) %>% 
    spread(lap, val) %>% 
    select(-team, -var)
  
  paste0(i, " = ", round(psych::alpha(x)$total$raw_alpha, 2))
})

# save distribution plots for manuscript
# library(cowplot)
# 
a <- vars %>%
  ggplot(aes(x = collisions_overall)) +
  geom_histogram(binwidth = 35, fill = "white", colour = "black") +
  scale_y_continuous(breaks = seq(0, 14, 2), limits = c(0, 14)) +
  # scale_x_continuous(breaks = seq(0, 600, 200), limits = c(0, 650)) +
  labs(y = "Frequency", x = "Collisions") +
  theme_classic()

b <- vars %>%
  ggplot(aes(x = speed_overall)) +
  geom_histogram(binwidth = .5, fill = "white", colour = "black") +
  scale_y_continuous(breaks = seq(0, 8, 2), limits = c(0, 8)) +
  scale_x_continuous(breaks = seq(4, 12, 2), limits = c(4, 12)) +
  labs(y = "", x = "Speed") +
  theme_classic()

c <- vars %>%
  ggplot(aes(x = distance_overall)) +
  geom_histogram(binwidth = 400, fill = "white", colour = "black") +
  scale_y_continuous(breaks = seq(0, 14, 2), limits = c(0, 14)) +
  scale_x_continuous(breaks = seq(10000, 18000, 2000), limits = c(10000, 18000)) +
  labs(y = "Frequency", x = "Distance travelled") +
  theme_classic()

p <- cowplot::plot_grid(a,b,c, ncol = 2, labels = c("A", "B", "C"),
               label_fontface = "bold", label_size = 12)

# ggsave(here("output/sim_metric_distributions.png"), width = 7, height = 5)

```

## Fog-free periods
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# No fog
# select variables
x <- vars %>% 
  select(collisions_no_fog_overall, speed_no_fog_overall, distance_no_fog_overall)

# descriptives
descriptives(x)

# distributions
distributions(x)

# correlations
kable(star_matrix(x))

# reliability estimates
name <- c("collisions_no_fog", "speed_no_fog", "distance_no_fog")

map(name, function(i) {
  x <- vars %>%
    ungroup() %>% 
    select(team, collisions_no_fog_1:collisions_no_fog_5, speed_no_fog_1:speed_no_fog_5, 
           distance_no_fog_1:distance_no_fog_5) %>% 
    gather(var, val, -team) %>% 
    mutate(lap = str_extract(var, "_[1-5]"),
           lap = as.numeric(str_remove(lap, "_")),
           var = str_remove(var, "_[1-5]")) %>% 
    filter(var == i) %>% 
    spread(lap, val) %>% 
    select(-team, -var)
  
  paste0(i, " = ", round(psych::alpha(x)$total$raw_alpha, 2))
})

```

## Fog event probe
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# Fog event
# select variables
x <- vars %>% 
  select(collisions_fog_overall, speed_fog_overall, distance_fog_overall)

# descriptives
descriptives(x)

# distributions
distributions(x)

# correlations
kable(star_matrix(x))

# reliability estimates
name <- c("collisions_fog", "speed_fog", "distance_fog")

map(name, function(i) {
  x <- vars %>%
    ungroup() %>% 
    select(team, collisions_fog_1:collisions_fog_5, speed_fog_1:speed_fog_5, 
           distance_fog_1:distance_fog_5) %>% 
    gather(var, val, -team) %>% 
    mutate(lap = str_extract(var, "_[1-5]"),
           lap = as.numeric(str_remove(lap, "_")),
           var = str_remove(var, "_[1-5]")) %>% 
    filter(var == i) %>% 
    spread(lap, val) %>% 
    select(-team, -var)
  
  paste0(i, " = ", round(psych::alpha(x)$total$raw_alpha, 2))
})

```

## Psychometric measures
## Overall
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# descriptives
# overall
o <- vars %>% 
  select(team, driving_years:neuroticism, driving_years_drone:neuroticism_drone) %>% 
  gather(var, val, -team) %>% 
  mutate(var = str_remove(var, "_drone"))

tmp1 <- o %>% 
  group_by(var) %>% 
  summarise(mean_overall = mean(val, na.rm = T),
            sd_overall = sd(val, na.rm = T)
            # min_overall = min(val, na.rm = T),
            # max_overall = max(val, na.rm = T)
            )

# driver
x <- vars %>% 
  select(driving_years:neuroticism)

tmp2 <- descriptives(x) %>% 
  select(-var, -min, -max) %>% 
  rename(mean_driver = mean, sd_driver = sd 
         # min_driver = min, max_driver = max
         )

# codriver
x <- vars %>% 
  select(driving_years_drone:neuroticism_drone)

tmp3 <- descriptives(x) %>% 
  select(-var, -min, -max) %>% 
  rename(mean_co = mean, sd_co = sd
         #min_co = min, max_co = max
         )

# print together in single table
kable(bind_cols(tmp1, tmp2, tmp3))

# save descriptives
# bind_cols(tmp1, tmp2, tmp3) %>% 
#   mutate_if(is.numeric, round, 2) %>% 
#   write_csv(here("output/psych_descriptives.csv"))

# overall distribution plots
o %>% 
  ggplot(aes(val)) +
  geom_histogram() +
  facet_wrap(~var, scales = "free")

# overall correlations
x <- vars %>% 
  select(uid, driving_years_drone:neuroticism_drone) %>% 
  gather(var, val, -uid) %>% 
  mutate(var = str_remove(var, "_drone")) %>% 
  spread(var, val) %>% 
  rbind(vars %>% 
  select(uid, driving_years:neuroticism)) %>% 
  select(-uid) %>% 
  star_matrix()

kable(x)

```

## Reliability overall
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# Reliability estimates for each psychometric measure overall
# read survey data
surveys <- readRDS(here("data/survey_items.rds")) 

all_id <- surveys$Demographics %>% 
  select(uid) %>% 
  mutate(team = str_replace(uid, "-.*", "")) %>% 
  filter(team %in% vars$team) %>% 
  select(-team)

# driver ids
drive_id <- vars %>% select(uid) %>% unique()

# codriver ids
co_id <- surveys$Demographics %>% 
  select(uid) %>% 
  mutate(team = str_replace(uid, "-.*", "")) %>% 
  filter(team %in% vars$team) %>% 
  filter(!uid %in% drive_id$uid) %>% 
  select(-team)

survey <- map(surveys, filter, uid %in% all_id$uid)

# Connor_Davidson_Resilience
x <- survey$Connor_Davidson_Resilience %>% 
  select(uid, item_num, response) %>% 
  spread(item_num, response) %>% 
  select(-uid)

print(paste0("resilience = ", round(psych::alpha(x)$total$raw_alpha,2)))

# mini-IPIP
fac <- unique(survey$Mini_IPIP$facet)

map(fac, function(i) {
  x <- survey$Mini_IPIP %>% 
    mutate(response = ifelse(key == "-1", 6-response, response)) %>% 
    filter(facet == i) %>% 
    select(uid, item_num, response) %>%
    spread(item_num, response) %>%
    select(-uid)
  
  print(paste0(i, " = ", round(psych::alpha(x)$total$raw_alpha,2)))
})

# RAPM
# accuracy
x <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  select(uid, ApmItemNum, Stimulus_ACC) %>% 
  mutate(Stimulus_ACC = as.numeric(Stimulus_ACC)) %>% 
  spread(ApmItemNum, Stimulus_ACC) %>% 
  select(-uid)

print(paste0("RAPM accuracy = ", round(psych::alpha(x)$total$raw_alpha,2)))

# confidence
x <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  select(uid, ApmItemNum, Confidence_RESP) %>% 
  spread(ApmItemNum, Confidence_RESP) %>% 
  select(-uid)

print(paste0("RAPM confidence = ", round(psych::alpha(x)$total$raw_alpha,2)))

# bias
# calculate bias for the odd and even items separately
# correlate the two sets then ajust with the Spearman-Brown prophecy formula
items <- survey$Ravens_Advanced_Progressive_Matrices %>% select(ApmItemNum) %>% unique()

odd <- items[c(seq(1, 20, 2)),]
even <- items[c(seq(2, 20, 2)),]

odd <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% odd$ApmItemNum) %>% 
  group_by(uid) %>% 
  summarise(acc = 100 * mean(Stimulus_ACC),
            conf = mean(Confidence_RESP),
            bias = conf - acc)

even <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% even$ApmItemNum) %>% 
  group_by(uid) %>% 
  summarise(acc = 100 * mean(Stimulus_ACC),
            conf = mean(Confidence_RESP),
            bias = conf - acc)

cor <- cor.test(odd$bias, even$bias)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("RAPM bias = ", round((2*cor$estimate) / (1+cor$estimate),2)))


# discrimination
# calculate for the odd and even items separately
# correlate the two sets then ajust with the Spearman-Brown prophecy formula
items <- survey$Ravens_Advanced_Progressive_Matrices %>% select(ApmItemNum) %>% unique()

odd <- items[c(seq(1, 20, 2)),]
even <- items[c(seq(2, 20, 2)),]

odd <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% odd$ApmItemNum) %>% 
  group_by(uid, Stimulus_ACC) %>% 
  summarise(conf = mean(Confidence_RESP, na.rm = T)) %>% 
  gather(var, val, conf) %>% 
  unite(var, var, Stimulus_ACC) %>% 
  spread(var, val) %>% 
  group_by(uid) %>% 
  summarise(discrim = conf_TRUE - conf_FALSE)

even <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% even$ApmItemNum) %>% 
  group_by(uid, Stimulus_ACC) %>% 
  summarise(conf = mean(Confidence_RESP, na.rm = T)) %>% 
  gather(var, val, conf) %>% 
  unite(var, var, Stimulus_ACC) %>% 
  spread(var, val) %>% 
  group_by(uid) %>% 
  summarise(discrim = conf_TRUE - conf_FALSE)

cor <- cor.test(odd$discrim, even$discrim)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("RAPM discrimination = ", round((2*cor$estimate) / (1+cor$estimate),2)))

# task switching ----------------------------------------------------------
# uneven number of repeat and switch trials for each participant so selected 
# a complete subset to calculate reliability: the largest number of trials 
# completed by all participants in each trial type

type <- unique(survey$Task_Switching$type)

# Accuracy
map(type, function(i) {
  x <- survey$Task_Switching %>%
    filter(type == i) %>%
    select(uid, Stimulus_ACC) %>%
    group_by(uid) %>%
    mutate(itemnum = row_number())
  
  if (i == "repeat") {
    x <- x %>% 
      filter(itemnum %in% c(1:26)) %>%
      select(uid, itemnum, Stimulus_ACC) %>%
      mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                   ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>%
      spread(itemnum, Stimulus_ACC) %>%
      ungroup() %>% 
      select(-uid)
  } 
  
  if (i == "switch") {
    x <- x %>% 
      filter(itemnum %in% c(1:24)) %>% 
      select(uid, itemnum, Stimulus_ACC) %>%
      mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                   ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>%
      spread(itemnum, Stimulus_ACC) %>%
      ungroup() %>% 
      select(-uid)
  }
  
  print(paste0(i, " errors = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# time
map(type, function(i) {
  x <- survey$Task_Switching %>%
    filter(type == i) %>%
    select(uid, Stimulus_RT) %>%
    group_by(uid) %>%
    mutate(itemnum = row_number())
  
  if (i == "repeat") {
    x <- x %>% 
      filter(itemnum %in% c(1:26)) %>%
      select(uid, itemnum, Stimulus_RT) %>%
      mutate(Stimulus_RT = ifelse(Stimulus_RT == TRUE, 1,
                                  ifelse(Stimulus_RT == FALSE, 0, Stimulus_RT))) %>%
      spread(itemnum, Stimulus_RT) %>%
      ungroup() %>% 
      select(-uid)
  } else {
    x <- x %>% 
      filter(itemnum %in% c(1:24)) %>% 
      select(uid, itemnum, Stimulus_RT) %>%
      mutate(Stimulus_RT = ifelse(Stimulus_RT == TRUE, 1,
                                  ifelse(Stimulus_RT == FALSE, 0, Stimulus_RT))) %>%
      spread(itemnum, Stimulus_RT) %>%
      ungroup() %>% 
      select(-uid)
  }
  
  print(paste0(i, " time = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# inhibitory cost
# prepare data
x <- survey$Task_Switching %>%
  select(uid, type, Stimulus_RT) %>%
  group_by(uid, type) %>%
  mutate(itemnum = row_number()) %>% 
  filter(itemnum %in% c(1:24))

# select odd and even items
odds <- seq(1,24,2)
evens <- seq(2,24,2)

# calculate reliability
odd <- x %>%
  filter(itemnum %in% odds) %>% 
  group_by(uid) %>% 
  summarise(cost1 = mean(Stimulus_RT[type=="switch"] - mean(Stimulus_RT[type=="repeat"])))

even <- x %>%
  filter(itemnum %in% evens) %>% 
  group_by(uid) %>% 
  summarise(cost2 = mean(Stimulus_RT[type=="switch"] - mean(Stimulus_RT[type=="repeat"])))

cor <- cor.test(odd$cost1, even$cost2)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("switch cost = ", round((2*cor$estimate) / (1+cor$estimate),2)))


# running letters ---------------------------------------------------------
# Accuracy
x <- survey$Running_Letters %>%
  select(uid, RUNNINGtest_TrialNr, Stimulus_ACC) %>%
  group_by(uid) %>%
  mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                               ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>% 
  spread(RUNNINGtest_TrialNr, Stimulus_ACC) %>%
  ungroup() %>% 
  select(-uid)

print(paste0("working memory accuracy = ", round(psych::alpha(x)$total$raw_alpha,2)))



# flanker task ------------------------------------------------------------
# 80 congruent trials (1 person with 79)
# 20 incongruent trials
type <- unique(survey$Flanker$Congruency)

# Accuracy
map(type, function(i) {
  x <- survey$Flanker %>%
    filter(Congruency == i) %>%
    select(uid, FLANKERtest_TrialNr, Stimulus_ACC) %>%
    group_by(uid) %>% 
    mutate(row = 1:n(),
           Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                 ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC)),
           Stimulus_ACC = as.numeric(Stimulus_ACC)) %>% 
    select(-FLANKERtest_TrialNr) %>% 
    spread(row, Stimulus_ACC) %>%
    ungroup() %>% 
    select(-uid)
  
  print(paste0(i, " errors = ", round(psych::alpha(x)$total$raw_alpha,2)))
})

# time
map(type, function(i) {
  x <- survey$Flanker %>%
    filter(Congruency == i) %>%
    select(uid, FLANKERtest_TrialNr, Stimulus_RT) %>%
    group_by(uid) %>% 
    mutate(row = 1:n()) %>% 
    select(-FLANKERtest_TrialNr) %>% 
    spread(row, Stimulus_RT) %>%
    ungroup() %>% 
    select(-uid)
  
  print(paste0(i, " time = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# switch cost
# prepare data
x <- survey$Flanker %>%
  select(uid, Congruency, Stimulus_RT) %>%
  group_by(uid, Congruency) %>%
  mutate(itemnum = row_number())

# select odd and even items
odd_con <- seq(1,80,2)
even_con <- seq(2,80,2)

odd_in <- seq(1,20,2)
even_in <- seq(2,20,2)

# calculate reliability
con_o <- x %>% 
  filter(itemnum %in% odd_con) %>% 
  group_by(uid) %>% 
  summarise(con1 = mean(Stimulus_RT))

in_o <- x %>% 
  filter(itemnum %in% odd_in) %>% 
  group_by(uid) %>% 
  summarise(in1 = mean(Stimulus_RT))

con_e <- x %>% 
  filter(itemnum %in% even_con) %>% 
  group_by(uid) %>% 
  summarise(con2 = mean(Stimulus_RT))

in_e <- x %>% 
  filter(itemnum %in% even_in) %>% 
  group_by(uid) %>% 
  summarise(in2 = mean(Stimulus_RT))


datalist <- list(con_o, in_o, con_e, in_e)

x <- datalist %>% reduce(left_join, by = "uid") %>% 
  group_by(uid) %>% 
  summarise(cost1 = con2 - con1,
            cost2 = in2 - in1)

cor <- cor.test(x$cost1, x$cost2)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("inhibitory cost = ", round((2*cor$estimate) / (1+cor$estimate),2)))

```
### Reliability for driver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# Reliability estimates for each psychometric measure driver
# filter survey data
survey <- map(surveys, filter, uid %in% drive_id$uid)

# Connor_Davidson_Resilience
x <- survey$Connor_Davidson_Resilience %>% 
  select(uid, item_num, response) %>% 
  spread(item_num, response) %>% 
  select(-uid)

print(paste0("resilience = ", round(psych::alpha(x)$total$raw_alpha,2)))

# mini-IPIP
fac <- unique(survey$Mini_IPIP$facet)

map(fac, function(i) {
  x <- survey$Mini_IPIP %>% 
    mutate(response = ifelse(key == "-1", 6-response, response)) %>% 
    filter(facet == i) %>% 
    select(uid, item_num, response) %>%
    spread(item_num, response) %>%
    select(-uid)
  
  print(paste0(i, " = ", round(psych::alpha(x)$total$raw_alpha,2)))
})

# RAPM
# accuracy
x <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  select(uid, ApmItemNum, Stimulus_ACC) %>% 
  mutate(Stimulus_ACC = as.numeric(Stimulus_ACC)) %>% 
  spread(ApmItemNum, Stimulus_ACC) %>% 
  select(-uid)

print(paste0("RAPM accuracy = ", round(psych::alpha(x)$total$raw_alpha,2)))

# confidence
x <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  select(uid, ApmItemNum, Confidence_RESP) %>% 
  spread(ApmItemNum, Confidence_RESP) %>% 
  select(-uid)

print(paste0("RAPM confidence = ", round(psych::alpha(x)$total$raw_alpha,2)))

# bias
# calculate bias for the odd and even items separately
# correlate the two sets then ajust with the Spearman-Brown prophecy formula
items <- survey$Ravens_Advanced_Progressive_Matrices %>% select(ApmItemNum) %>% unique()

odd <- items[c(seq(1, 20, 2)),]
even <- items[c(seq(2, 20, 2)),]

odd <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% odd$ApmItemNum) %>% 
  group_by(uid) %>% 
  summarise(acc = 100 * mean(Stimulus_ACC),
            conf = mean(Confidence_RESP),
            bias = conf - acc)

even <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% even$ApmItemNum) %>% 
  group_by(uid) %>% 
  summarise(acc = 100 * mean(Stimulus_ACC),
            conf = mean(Confidence_RESP),
            bias = conf - acc)

cor <- cor.test(odd$bias, even$bias)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("RAPM bias = ", round((2*cor$estimate) / (1+cor$estimate),2)))


# discrimination
# calculate for the odd and even items separately
# correlate the two sets then ajust with the Spearman-Brown prophecy formula
items <- survey$Ravens_Advanced_Progressive_Matrices %>% select(ApmItemNum) %>% unique()

odd <- items[c(seq(1, 20, 2)),]
even <- items[c(seq(2, 20, 2)),]

odd <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% odd$ApmItemNum) %>% 
  group_by(uid, Stimulus_ACC) %>% 
  summarise(conf = mean(Confidence_RESP, na.rm = T)) %>% 
  gather(var, val, conf) %>% 
  unite(var, var, Stimulus_ACC) %>% 
  spread(var, val) %>% 
  group_by(uid) %>% 
  summarise(discrim = conf_TRUE - conf_FALSE)

even <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% even$ApmItemNum) %>% 
  group_by(uid, Stimulus_ACC) %>% 
  summarise(conf = mean(Confidence_RESP, na.rm = T)) %>% 
  gather(var, val, conf) %>% 
  unite(var, var, Stimulus_ACC) %>% 
  spread(var, val) %>% 
  group_by(uid) %>% 
  summarise(discrim = conf_TRUE - conf_FALSE)

cor <- cor.test(odd$discrim, even$discrim)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("RAPM discrimination = ", round((2*cor$estimate) / (1+cor$estimate),2)))

# task switching ----------------------------------------------------------
# uneven number of repeat and switch trials for each participant so selected 
# a complete subset to calculate reliability: the largest number of trials 
# completed by all participants in each trial type

type <- unique(survey$Task_Switching$type)

# Accuracy
map(type, function(i) {
  x <- survey$Task_Switching %>%
    filter(type == i) %>%
    select(uid, Stimulus_ACC) %>%
    group_by(uid) %>%
    mutate(itemnum = row_number())
  
  if (i == "repeat") {
    x <- x %>% 
      filter(itemnum %in% c(1:26)) %>%
      select(uid, itemnum, Stimulus_ACC) %>%
      mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                   ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>%
      spread(itemnum, Stimulus_ACC) %>%
      ungroup() %>% 
      select(-uid)
  } 
  
  if (i == "switch") {
    x <- x %>% 
      filter(itemnum %in% c(1:24)) %>% 
      select(uid, itemnum, Stimulus_ACC) %>%
      mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                   ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>%
      spread(itemnum, Stimulus_ACC) %>%
      ungroup() %>% 
      select(-uid)
  }
  
  print(paste0(i, " errors = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# time
map(type, function(i) {
  x <- survey$Task_Switching %>%
    filter(type == i) %>%
    select(uid, Stimulus_RT) %>%
    group_by(uid) %>%
    mutate(itemnum = row_number())
  
  if (i == "repeat") {
    x <- x %>% 
      filter(itemnum %in% c(1:26)) %>%
      select(uid, itemnum, Stimulus_RT) %>%
      mutate(Stimulus_RT = ifelse(Stimulus_RT == TRUE, 1,
                                  ifelse(Stimulus_RT == FALSE, 0, Stimulus_RT))) %>%
      spread(itemnum, Stimulus_RT) %>%
      ungroup() %>% 
      select(-uid)
  } else {
    x <- x %>% 
      filter(itemnum %in% c(1:24)) %>% 
      select(uid, itemnum, Stimulus_RT) %>%
      mutate(Stimulus_RT = ifelse(Stimulus_RT == TRUE, 1,
                                  ifelse(Stimulus_RT == FALSE, 0, Stimulus_RT))) %>%
      spread(itemnum, Stimulus_RT) %>%
      ungroup() %>% 
      select(-uid)
  }
  
  print(paste0(i, " time = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# inhibitory cost
# prepare data
x <- survey$Task_Switching %>%
  select(uid, type, Stimulus_RT) %>%
  group_by(uid, type) %>%
  mutate(itemnum = row_number()) %>% 
  filter(itemnum %in% c(1:24))

# select odd and even items
odds <- seq(1,24,2)
evens <- seq(2,24,2)

# calculate reliability
odd <- x %>%
  filter(itemnum %in% odds) %>% 
  group_by(uid) %>% 
  summarise(cost1 = mean(Stimulus_RT[type=="switch"] - mean(Stimulus_RT[type=="repeat"])))

even <- x %>%
  filter(itemnum %in% evens) %>% 
  group_by(uid) %>% 
  summarise(cost2 = mean(Stimulus_RT[type=="switch"] - mean(Stimulus_RT[type=="repeat"])))

cor <- cor.test(odd$cost1, even$cost2)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("switch cost = ", round((2*cor$estimate) / (1+cor$estimate),2)))


# running letters ---------------------------------------------------------
# Accuracy
x <- survey$Running_Letters %>%
  select(uid, RUNNINGtest_TrialNr, Stimulus_ACC) %>%
  group_by(uid) %>%
  mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                               ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>% 
  spread(RUNNINGtest_TrialNr, Stimulus_ACC) %>%
  ungroup() %>% 
  select(-uid)

print(paste0("working memory accuracy = ", round(psych::alpha(x)$total$raw_alpha,2)))



# flanker task ------------------------------------------------------------
# 80 congruent trials (1 person with 79)
# 20 incongruent trials
type <- unique(survey$Flanker$Congruency)

# Accuracy
map(type, function(i) {
  x <- survey$Flanker %>%
    filter(Congruency == i) %>%
    select(uid, FLANKERtest_TrialNr, Stimulus_ACC) %>%
    group_by(uid) %>% 
    mutate(row = 1:n(),
           Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                 ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC)),
           Stimulus_ACC = as.numeric(Stimulus_ACC)) %>% 
    select(-FLANKERtest_TrialNr) %>% 
    spread(row, Stimulus_ACC) %>%
    ungroup() %>% 
    select(-uid)
  
  print(paste0(i, " errors = ", round(psych::alpha(x)$total$raw_alpha,2)))
})

# time
map(type, function(i) {
  x <- survey$Flanker %>%
    filter(Congruency == i) %>%
    select(uid, FLANKERtest_TrialNr, Stimulus_RT) %>%
    group_by(uid) %>% 
    mutate(row = 1:n()) %>% 
    select(-FLANKERtest_TrialNr) %>% 
    spread(row, Stimulus_RT) %>%
    ungroup() %>% 
    select(-uid)
  
  print(paste0(i, " time = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# switch cost
# prepare data
x <- survey$Flanker %>%
  select(uid, Congruency, Stimulus_RT) %>%
  group_by(uid, Congruency) %>%
  mutate(itemnum = row_number())

# select odd and even items
odd_con <- seq(1,80,2)
even_con <- seq(2,80,2)

odd_in <- seq(1,20,2)
even_in <- seq(2,20,2)

# calculate reliability
con_o <- x %>% 
  filter(itemnum %in% odd_con) %>% 
  group_by(uid) %>% 
  summarise(con1 = mean(Stimulus_RT))

in_o <- x %>% 
  filter(itemnum %in% odd_in) %>% 
  group_by(uid) %>% 
  summarise(in1 = mean(Stimulus_RT))

con_e <- x %>% 
  filter(itemnum %in% even_con) %>% 
  group_by(uid) %>% 
  summarise(con2 = mean(Stimulus_RT))

in_e <- x %>% 
  filter(itemnum %in% even_in) %>% 
  group_by(uid) %>% 
  summarise(in2 = mean(Stimulus_RT))


datalist <- list(con_o, in_o, con_e, in_e)

x <- datalist %>% reduce(left_join, by = "uid") %>% 
  group_by(uid) %>% 
  summarise(cost1 = con2 - con1,
            cost2 = in2 - in1)

cor <- cor.test(x$cost1, x$cost2)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("inhibitory cost = ", round((2*cor$estimate) / (1+cor$estimate),2)))

```

### Reliability for codriver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# Reliability estimates for each psychometric measure codriver
# filter survey data
survey <- map(surveys, filter, uid %in% co_id$uid)

# Connor_Davidson_Resilience
x <- survey$Connor_Davidson_Resilience %>% 
  select(uid, item_num, response) %>% 
  spread(item_num, response) %>% 
  select(-uid)

print(paste0("resilience = ", round(psych::alpha(x)$total$raw_alpha,2)))

# mini-IPIP
fac <- unique(survey$Mini_IPIP$facet)

map(fac, function(i) {
  x <- survey$Mini_IPIP %>% 
    mutate(response = ifelse(key == "-1", 6-response, response)) %>% 
    filter(facet == i) %>% 
    select(uid, item_num, response) %>%
    spread(item_num, response) %>%
    select(-uid)
  
  print(paste0(i, " = ", round(psych::alpha(x)$total$raw_alpha,2)))
})

# RAPM
# accuracy
x <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  select(uid, ApmItemNum, Stimulus_ACC) %>% 
  mutate(Stimulus_ACC = as.numeric(Stimulus_ACC)) %>% 
  spread(ApmItemNum, Stimulus_ACC) %>% 
  select(-uid)

print(paste0("RAPM accuracy = ", round(psych::alpha(x)$total$raw_alpha,2)))

# confidence
x <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  select(uid, ApmItemNum, Confidence_RESP) %>% 
  spread(ApmItemNum, Confidence_RESP) %>% 
  select(-uid)

print(paste0("RAPM confidence = ", round(psych::alpha(x)$total$raw_alpha,2)))

# bias
# calculate bias for the odd and even items separately
# correlate the two sets then ajust with the Spearman-Brown prophecy formula
items <- survey$Ravens_Advanced_Progressive_Matrices %>% select(ApmItemNum) %>% unique()

odd <- items[c(seq(1, 20, 2)),]
even <- items[c(seq(2, 20, 2)),]

odd <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% odd$ApmItemNum) %>% 
  group_by(uid) %>% 
  summarise(acc = 100 * mean(Stimulus_ACC),
            conf = mean(Confidence_RESP),
            bias = conf - acc)

even <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% even$ApmItemNum) %>% 
  group_by(uid) %>% 
  summarise(acc = 100 * mean(Stimulus_ACC),
            conf = mean(Confidence_RESP),
            bias = conf - acc)

cor <- cor.test(odd$bias, even$bias)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("RAPM bias = ", round((2*cor$estimate) / (1+cor$estimate),2)))


# discrimination
# calculate for the odd and even items separately
# correlate the two sets then ajust with the Spearman-Brown prophecy formula
items <- survey$Ravens_Advanced_Progressive_Matrices %>% select(ApmItemNum) %>% unique()

odd <- items[c(seq(1, 20, 2)),]
even <- items[c(seq(2, 20, 2)),]

odd <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% odd$ApmItemNum) %>% 
  group_by(uid, Stimulus_ACC) %>% 
  summarise(conf = mean(Confidence_RESP, na.rm = T)) %>% 
  gather(var, val, conf) %>% 
  unite(var, var, Stimulus_ACC) %>% 
  spread(var, val) %>% 
  group_by(uid) %>% 
  summarise(discrim = conf_TRUE - conf_FALSE)

even <- survey$Ravens_Advanced_Progressive_Matrices %>% 
  filter(ApmItemNum %in% even$ApmItemNum) %>% 
  group_by(uid, Stimulus_ACC) %>% 
  summarise(conf = mean(Confidence_RESP, na.rm = T)) %>% 
  gather(var, val, conf) %>% 
  unite(var, var, Stimulus_ACC) %>% 
  spread(var, val) %>% 
  group_by(uid) %>% 
  summarise(discrim = conf_TRUE - conf_FALSE)

cor <- cor.test(odd$discrim, even$discrim)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("RAPM discrimination = ", round((2*cor$estimate) / (1+cor$estimate),2)))

# task switching ----------------------------------------------------------
# uneven number of repeat and switch trials for each participant so selected 
# a complete subset to calculate reliability: the largest number of trials 
# completed by all participants in each trial type

type <- unique(survey$Task_Switching$type)

# Accuracy
map(type, function(i) {
  x <- survey$Task_Switching %>%
    filter(type == i) %>%
    select(uid, Stimulus_ACC) %>%
    group_by(uid) %>%
    mutate(itemnum = row_number())
  
  if (i == "repeat") {
    x <- x %>% 
      filter(itemnum %in% c(1:26)) %>%
      select(uid, itemnum, Stimulus_ACC) %>%
      mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                   ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>%
      spread(itemnum, Stimulus_ACC) %>%
      ungroup() %>% 
      select(-uid)
  } 
  
  if (i == "switch") {
    x <- x %>% 
      filter(itemnum %in% c(1:24)) %>% 
      select(uid, itemnum, Stimulus_ACC) %>%
      mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                   ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>%
      spread(itemnum, Stimulus_ACC) %>%
      ungroup() %>% 
      select(-uid)
  }
  
  print(paste0(i, " errors = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# time
map(type, function(i) {
  x <- survey$Task_Switching %>%
    filter(type == i) %>%
    select(uid, Stimulus_RT) %>%
    group_by(uid) %>%
    mutate(itemnum = row_number())
  
  if (i == "repeat") {
    x <- x %>% 
      filter(itemnum %in% c(1:26)) %>%
      select(uid, itemnum, Stimulus_RT) %>%
      mutate(Stimulus_RT = ifelse(Stimulus_RT == TRUE, 1,
                                  ifelse(Stimulus_RT == FALSE, 0, Stimulus_RT))) %>%
      spread(itemnum, Stimulus_RT) %>%
      ungroup() %>% 
      select(-uid)
  } else {
    x <- x %>% 
      filter(itemnum %in% c(1:24)) %>% 
      select(uid, itemnum, Stimulus_RT) %>%
      mutate(Stimulus_RT = ifelse(Stimulus_RT == TRUE, 1,
                                  ifelse(Stimulus_RT == FALSE, 0, Stimulus_RT))) %>%
      spread(itemnum, Stimulus_RT) %>%
      ungroup() %>% 
      select(-uid)
  }
  
  print(paste0(i, " time = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# inhibitory cost
# prepare data
x <- survey$Task_Switching %>%
  select(uid, type, Stimulus_RT) %>%
  group_by(uid, type) %>%
  mutate(itemnum = row_number()) %>% 
  filter(itemnum %in% c(1:24))

# select odd and even items
odds <- seq(1,24,2)
evens <- seq(2,24,2)

# calculate reliability
odd <- x %>%
  filter(itemnum %in% odds) %>% 
  group_by(uid) %>% 
  summarise(cost1 = mean(Stimulus_RT[type=="switch"] - mean(Stimulus_RT[type=="repeat"])))

even <- x %>%
  filter(itemnum %in% evens) %>% 
  group_by(uid) %>% 
  summarise(cost2 = mean(Stimulus_RT[type=="switch"] - mean(Stimulus_RT[type=="repeat"])))

cor <- cor.test(odd$cost1, even$cost2)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("switch cost = ", round((2*cor$estimate) / (1+cor$estimate),2)))


# running letters ---------------------------------------------------------
# Accuracy
x <- survey$Running_Letters %>%
  select(uid, RUNNINGtest_TrialNr, Stimulus_ACC) %>%
  group_by(uid) %>%
  mutate(Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                               ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC))) %>% 
  spread(RUNNINGtest_TrialNr, Stimulus_ACC) %>%
  ungroup() %>% 
  select(-uid)

print(paste0("working memory accuracy = ", round(psych::alpha(x)$total$raw_alpha,2)))



# flanker task ------------------------------------------------------------
# 80 congruent trials (1 person with 79)
# 20 incongruent trials
type <- unique(survey$Flanker$Congruency)

# Accuracy
map(type, function(i) {
  x <- survey$Flanker %>%
    filter(Congruency == i) %>%
    select(uid, FLANKERtest_TrialNr, Stimulus_ACC) %>%
    group_by(uid) %>% 
    mutate(row = 1:n(),
           Stimulus_ACC = ifelse(Stimulus_ACC == TRUE, 1,
                                 ifelse(Stimulus_ACC == FALSE, 0, Stimulus_ACC)),
           Stimulus_ACC = as.numeric(Stimulus_ACC)) %>% 
    select(-FLANKERtest_TrialNr) %>% 
    spread(row, Stimulus_ACC) %>%
    ungroup() %>% 
    select(-uid)
  
  print(paste0(i, " errors = ", round(psych::alpha(x)$total$raw_alpha,2)))
})

# time
map(type, function(i) {
  x <- survey$Flanker %>%
    filter(Congruency == i) %>%
    select(uid, FLANKERtest_TrialNr, Stimulus_RT) %>%
    group_by(uid) %>% 
    mutate(row = 1:n()) %>% 
    select(-FLANKERtest_TrialNr) %>% 
    spread(row, Stimulus_RT) %>%
    ungroup() %>% 
    select(-uid)
  
  print(paste0(i, " time = ", round(psych::alpha(x)$total$raw_alpha,2)))
})


# switch cost
# prepare data
x <- survey$Flanker %>%
  select(uid, Congruency, Stimulus_RT) %>%
  group_by(uid, Congruency) %>%
  mutate(itemnum = row_number())

# select odd and even items
odd_con <- seq(1,80,2)
even_con <- seq(2,80,2)

odd_in <- seq(1,20,2)
even_in <- seq(2,20,2)

# calculate reliability
con_o <- x %>% 
  filter(itemnum %in% odd_con) %>% 
  group_by(uid) %>% 
  summarise(con1 = mean(Stimulus_RT))

in_o <- x %>% 
  filter(itemnum %in% odd_in) %>% 
  group_by(uid) %>% 
  summarise(in1 = mean(Stimulus_RT))

con_e <- x %>% 
  filter(itemnum %in% even_con) %>% 
  group_by(uid) %>% 
  summarise(con2 = mean(Stimulus_RT))

in_e <- x %>% 
  filter(itemnum %in% even_in) %>% 
  group_by(uid) %>% 
  summarise(in2 = mean(Stimulus_RT))


datalist <- list(con_o, in_o, con_e, in_e)

x <- datalist %>% reduce(left_join, by = "uid") %>% 
  group_by(uid) %>% 
  summarise(cost1 = con2 - con1,
            cost2 = in2 - in1)

cor <- cor.test(x$cost1, x$cost2)

# Adjust with the Spearman-Brown prophecy formula = (2*r) / (1+r)  
print(paste0("inhibitory cost = ", round((2*cor$estimate) / (1+cor$estimate),2)))

```


## Communication measures
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# select variables
x <- vars %>% 
  select(co_info_help_overall:drive_frust_overall, -contains("ratio"), -contains("total"))

# descriptives
descriptives(x)

# distributions
distributions(x)

# correlations
kable(star_matrix(x))

```

## Reliability estimates
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# reliability for each comms variable
name <- names(vars %>% select(co_info_help_overall:drive_frust_overall, -contains("ratio"), -contains("total")))
              
name <- gsub("_overall", "", name)

map(name, function(i) {
  x <- vars %>%
    ungroup() %>% 
    select(team, co_info_harm_1:drive_total_5) %>% 
    gather(var, val, -team) %>% 
    mutate(lap = str_extract(var, "_[1-5]"),
           lap = as.numeric(str_remove(lap, "_")),
           var = str_remove(var, "_[1-5]")) %>% 
    filter(var == i) %>% 
    spread(lap, val) %>% 
    select(-team, -var)
  
  paste0(i, " = ", round(psych::alpha(x)$total$raw_alpha, 2))
})
```

## 
Given the moderate to high correlations between the communication variables, it looks like there is a multicollinearity issue. Need to reduce comms variables to a smaller number using factor analysis.

# Conduct EFA to extract comms factors
Reduce variables to smaller number of factors and correlate with each other. It looks like 3 components can be extracted from the comms variables.

## Correlations between original comms variables
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# select original comms variables
pca <- vars %>% dplyr::select(co_info_help_overall:drive_frust_overall, 
                -contains("ratio"), -co_total_help_overall, 
                -co_total_harm_overall, -co_total_overall)

# create the correlation matrix for PCA so we know how it was done (e.g., how missing values were treated)
cor_pca <- star_matrix(pca)

# print correlations
kable(cor_pca)

# Visualise correlations to see if variables appear to cluster
corrplot(cor(pca, use="complete.obs"), order = "hclust", tl.col='black', tl.cex=.75)

```

## KMO and Bartlett's test of Spherecity
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# Kaiser-Meyer-Olkin Measure of Sampling Adequacy (KMO)
print(KMO(cor(pca, use="complete.obs")))

# Bartlett's test of spherecity
print("Bartletts test of spherecity")
print(data.frame(cortest.bartlett(cor(pca, use="complete.obs"), n = 55)))
```

## Communalities
``` {r echo=FALSE, message=FALSE, warning=FALSE}
library(GPArotation)

# 3-component PCA
n_comp <- 3
rotate_method <- "promax"
score_method <- "Bartlett"

fit <- principal(pca, rotate = rotate_method, nfactors = n_comp, 
                        method = score_method, scores = TRUE, n.obs = 55)

# communalities
x <- data.frame(communalities = round(fit$communality, 2))

kable(x)

```

## Variance explained by the extracted components
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# variance explained
x <- data.frame(component = 1:9,
           eigen = fit$values,
           prop_var = c(fit$Vaccounted[2,c(1:3)], rnorm(6, 0, 0)),
           cum_var = c(fit$Vaccounted[3,c(1:3)], rnorm(6, 0, 0)),
           rotation_SS_load = c(fit$Vaccounted[1,c(1:3)], rnorm(6, 0, 0))) %>% 
  round(2)

x[x == 0] <- ""

kable(x)

# Scree plot
p <- data.frame(component = 1:9,
           eigen = fit$values)

p %>% ggplot(aes(x = component, y = eigen)) +
  geom_line() + geom_point() +
  scale_x_continuous(breaks = 1:9) +
  theme_minimal() +
  labs(title = "Scree plot") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Pattern matrix
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# pattern matrix
load <- data.frame(var = rownames(fit$loadings),
                   PC1 = round(fit$loadings[1:9], 2),
                   PC2 = round(fit$loadings[10:18], 2),
                   PC3 = round(fit$loadings[19:27], 2)) %>% 
  mutate(PC1 = ifelse(PC1 < .3 & PC1 > -.3, "", PC1),
         PC2 = ifelse(PC2 < .3 & PC2 > -.3, "", PC2),
         PC3 = ifelse(PC3 < .3 & PC3 > -.3, "", PC3)) %>% 
  arrange(desc(PC1), desc(PC2), desc(PC3))
  
kable(load)

```

## Component correlations matrix
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# Component correlations matrix
rownames(fit$r.scores) <- c("inconsistent", "terrible", "helpful")
colnames(fit$r.scores) <- c("inconsistent", "terrible", "helpful")

round(fit$r.scores,2)

```

## Check that component scores extracted using SPSS and R are the same
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# save component scores as dataframe
pca_scores <- data.frame(fit$scores) %>% 
  rename(inconsistent_codriver_r = RC1, terrible_codriver_r = RC2, helpful_exchange_r = RC3)

# append component scores to dataset
vars <- cbind(vars, pca_scores)

# check that spss and r component scores are the same
cor1 <- cor.test(vars$inconsistent_codriver, vars$inconsistent_codriver_r)
cor2 <- cor.test(vars$terrible_codriver, vars$terrible_codriver_r)
cor3 <- cor.test(vars$helpful_exchange, vars$helpful_exchange_r)

data.frame(component = c("inconsistent_codriver", "terrible_codriver", "helpful_exchange"),
           r = round(c(cor1$estimate, cor2$estimate, cor3$estimate),4))

```

# reduce NASA-TLX variables to smaller number of components
``` {r echo=FALSE, message=FALSE, warning=FALSE}

# select nasa variables
pca1 <- vars %>% dplyr::select(effort:temporal_demand)
pca2 <- vars %>% dplyr::select(effort_other:temporal_demand_other)
pca3 <- vars %>% dplyr::select(effort_drone:temporal_demand_drone)
pca4 <- vars %>% dplyr::select(effort_other_drone:temporal_demand_other_drone)

pca <- c("pca1", "pca2", "pca3", "pca4")

pca_scores <- map(pca, function(i) {
# create the correlation matrix for PCA so we know how it was done (e.g., how missing values were treated)
cor_pca <- star_matrix(get(i))

# print correlations
kable(cor_pca)

# Visualise correlations to see if variables appear to cluster
corrplot(cor(get(i), use="complete.obs"), order = "hclust", tl.col='black', tl.cex=.75)


# Kaiser-Meyer-Olkin Measure of Sampling Adequacy (KMO)
print(KMO(cor(get(i), use="complete.obs")))

# Bartlett's test of spherecity
print("Bartletts test of spherecity")
print(data.frame(cortest.bartlett(cor(get(i), use="complete.obs"), n = 55)))

# 3-component PCA
n_comp <- 2
rotate_method <- "promax"
score_method <- "Bartlett"

fit <- principal(get(i), rotate = rotate_method, nfactors = n_comp, 
                 method = score_method, scores = TRUE, n.obs = 55)

# communalities
x <- data.frame(communalities = round(fit$communality, 2))

kable(x)


# variance explained
x <- data.frame(component = 1:nrow(x),
                eigen = fit$values,
                prop_var = c(fit$Vaccounted[2,c(1:n_comp)], rnorm(nrow(x)-n_comp, 0, 0)),
                cum_var = c(fit$Vaccounted[3,c(1:n_comp)], rnorm(nrow(x)-n_comp, 0, 0)),
                rotation_SS_load = c(fit$Vaccounted[1,c(1:n_comp)], rnorm(nrow(x)-n_comp, 0, 0))) %>% 
  round(2)

x[x == 0] <- ""

kable(x)

# Scree plot
p <- data.frame(component = 1:nrow(x),
                eigen = fit$values)

p %>% ggplot(aes(x = component, y = eigen)) +
  geom_line() + geom_point() +
  scale_x_continuous(breaks = 1:nrow(x)) +
  theme_minimal() +
  labs(title = "Scree plot") +
  theme(plot.title = element_text(hjust = 0.5))


# pattern matrix
load <- data.frame(var = rownames(fit$loadings),
                   PC1 = round(fit$loadings[1:nrow(x)], 2),
                   PC2 = round(fit$loadings[(1+nrow(x)):(nrow(x)*2)], 2)) %>% 
  mutate(PC1 = ifelse(PC1 < .3 & PC1 > -.3, "", PC1),
         PC2 = ifelse(PC2 < .3 & PC2 > -.3, "", PC2)) %>% 
  arrange(desc(PC1), desc(PC2))

kable(load)




if (i == "pca1") {
  # Component correlations matrix
  rownames(fit$r.scores) <- c("positive_nasa", "negative_nasa")
  colnames(fit$r.scores) <- c("positive_nasa", "negative_nasa")

  round(fit$r.scores,2)

  # save component scores as dataframe
  pca_scores <- data.frame(fit$scores) %>%
    rename(positive_nasa_driver = RC1, negative_nasa_driver = RC2)

} else if (i == "pca2") {
  # Component correlations matrix
  rownames(fit$r.scores) <- c("positive_nasa", "negative_nasa")
  colnames(fit$r.scores) <- c("positive_nasa", "negative_nasa")

  round(fit$r.scores,2)

  # save component scores as dataframe
  pca_scores <- data.frame(fit$scores) %>%
    rename(positive_nasa_driver_other = RC1, negative_nasa_driver_other = RC2)

} else if (i == "pca3") {
# Component correlations matrix
rownames(fit$r.scores) <- c("negative_nasa", "positive_nasa")
colnames(fit$r.scores) <- c("negative_nasa", "positive_nasa")

round(fit$r.scores,2)


# save component scores as dataframe
pca_scores <- data.frame(fit$scores) %>%
  rename(negative_nasa_drone = RC1, positive_nasa_drone = RC2)

} else if (i == "pca4") {
  # Component correlations matrix
  rownames(fit$r.scores) <- c("negative_nasa", "positive_nasa")
  colnames(fit$r.scores) <- c("negative_nasa", "positive_nasa")

  round(fit$r.scores,2)


  # save component scores as dataframe
  pca_scores <- data.frame(fit$scores) %>%
    rename(negative_nasa_drone_other = RC1, positive_nasa_drone_other = RC2)
  
  }
})


# append component scores to dataset
pca_scores <- bind_cols(pca_scores)

vars <- cbind(vars, pca_scores)

```



# Regression analyses
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# functions to fit regression models and check assumptions + multicollinearity

# function to print sig correlations between IVs and DVs
sig_corrs <- function(data, dv) {
  # store correlations as df and convert to long format
  r <- data.frame(data$r) %>% 
    mutate(rowname = rownames(data$r)) %>% 
    gather(var , r, -rowname) %>% 
    mutate(r = round(r, 2))
  
  # store p-values as df and convert to long format
  p <- data.frame(data$p) %>% 
    mutate(rowname = rownames(data$p)) %>% 
    gather(var , p, -rowname)
  
  # combine r and p and print sig. correlations for each comms factor
  map(dv, function(i) {
    r %>% 
      left_join(p, by = c("rowname", "var")) %>% 
      filter(var %in% i) %>% 
      filter(!rowname %in% dv) %>% 
      filter(p < .05 & r != 1) %>% 
      select(-p) %>% 
      spread(var, r)  
  })
}

# function to select variables for the regression models
select_vars <- function(dv, sex = FALSE) {
  if(sex == FALSE) {
    a <- bind_rows(x) %>% 
      select(rowname, dv) %>% 
      na.omit() %>% select(rowname) %>% 
      filter(!rowname %in% c("sex_driver", "sex_co_driver", "sit.awareness_driver", 
                             "sit.awareness_co_driver", "sit.awareness",
                             "leadership_driver", "leadership_co_driver", "leadership"))
  } else if (sex == TRUE) {
    a <- bind_rows(x) %>% 
      select(rowname, dv) %>% 
      na.omit() %>% select(rowname) %>% 
      filter(!rowname %in% c("prop_female", "sit.awareness_driver", 
                             "sit.awareness_co_driver", "sit.awareness",
                             "leadership_driver", "leadership_co_driver", "leadership"))
  }
  if (all(c("aus_born_co_driver", "eng_fl_co_driver") %in% a$rowname)) {
    vars %>% select(a$rowname, -aus_born_co_driver)
  } else {
    vars %>% select(a$rowname)
  }
}


# function to fit regression model, and run diagnostics for overall simulation-derived metrics
regress <- function(dv, sex = FALSE, comm = FALSE) {
  # print dv name
  print(paste0("DV = ", dv))
  n <- names(var)
  
  if(all(comm == TRUE, sex == FALSE)) {
    comm_var <- n[n %in% c("eng_fl_co_driver", "eng_fl_driver")]
    n <- n[!n %in% c("eng_fl_co_driver", "eng_fl_driver")]
    
    # make sex a factor and standardise other variables  
    if (comm_var == "eng_fl_driver") {
      std_var <- var %>%
        mutate(eng_fl_driver = factor(eng_fl_driver)) %>%
        mutate_if(is.numeric, scale, center=TRUE, scale=TRUE)
    } else if (comm_var == "eng_fl_co_driver") {
      std_var <- var %>%
        mutate(eng_fl_co_driver = factor(eng_fl_co_driver)) %>%
        mutate_if(is.numeric, scale, center=TRUE, scale=TRUE)
    }
    
    print(psych::describe(std_var))
    
    # create regression formula
    fm <- as.formula(paste0("vars$", dv, " ~ ", paste0("scale(", n, ")", collapse = " + "), " + ", comm_var))
    
    # fit model
    f <- lm(fm, data = std_var)
    
  } else if(all(sex == FALSE, comm == FALSE)) {
    # create regression formula
    fm <- as.formula(paste0("vars$", dv, " ~ ", paste("scale(", n, ")", collapse = " + ")))
    
    # fit model
    f <- lm(fm, data = var)
    
  } else if(all(sex == TRUE, comm == FALSE)) {
    sex_var <- n[n %in% c("sex_driver", "sex_co_driver")]
    n <- n[!n %in% c("sex_driver", "sex_co_driver")]
    
    # make sex a factor and standardise other variables  
    if (sex_var == "sex_driver") {
      std_var <- var %>%
        mutate(sex_driver = factor(sex_driver)) %>%
        mutate_if(is.numeric, scale, center=TRUE, scale=TRUE)
    } else if (sex_var == "sex_co_driver") {
      std_var <- var %>%
        mutate(sex_co_driver = factor(sex_co_driver)) %>%
        mutate_if(is.numeric, scale, center=TRUE, scale=TRUE)
    }
    
    print(psych::describe(std_var))
    
    # create regression formula
    fm <- as.formula(paste0("vars$", dv, " ~ ", paste0("scale(", n, ")", collapse = " + "), " + ", sex_var))
    
    # fit model
    f <- lm(fm, data = std_var)
  }
  
  # results
  print(summary(f))
  
  # print zero-order, semi-part and partial correlations
  data <- cbind(vars %>% select(dv), var) %>% na.omit()
  library(ppcor) # package requires MASS which removes the select function from dplyr
  r <- cor(data)
  part <- pcor(data)
  spart <- spcor(data)
  cor <- data.frame(zero_order = round(r[1,c(2:length(data))],2),
                    partial = round(part[[1]][1,c(2:length(data))],2),
                    part = round(spart[[1]][1,c(2:length(data))],2))
  
  print(cor)
  
  # detach packages and reload dplyr
  detach("package:ppcor", unload=TRUE)
  detach("package:MASS", unload=TRUE)
  library(dplyr)
  
  # scatterplots of relationship between each IV and the DV
  p <- vars %>% select(dv, names(var)) %>% 
    gather(var, val, names(var)) %>% 
    ggplot(aes(x = val, y = get(dv))) +
    geom_point() +
    facet_wrap(~var, scales = "free_x") +
    geom_smooth(method='lm', formula = y~x, se=FALSE) +
    theme_minimal() +
    labs(x = "", y = str_replace(dv, "_", " "))
  
  print(p)
  
  # check assumptions and multicollinearity
  plot(f)
  
  # plot correlations between IVs
  p <- vars %>% select(names(var)) %>% 
    ggcorr(method = "pairwise.complete.obs", 
           label = TRUE, label_size = 3, label_round = 2, label_alpha = TRUE, # add text correlations value to plot
           hjust = 0.9, size = 4, color = "grey50", layout.exp = 3) # move variable labels
  # more detailed plot: distribution, scatterplot, correlations
  # vars %>% select(x, issue) %>% 
  #   ggpairs(., lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1)))
  
  # print correlation plot
  print(p)

  # check for multicollinearity
  if (length(var) > 1) {
    print(omcdiag(vars %>% select(names(var)), vars %>% select(dv)))
    
    print(imcdiag(vars %>% select(names(var)), vars %>% select(dv)))
  }
}

# function for adding interactions to regression iteratively
interact <- function(dv, sex = "prop_female") {
  map(comms_fac, function(i) {
    if (all(c(i, sex) %in% names(var))) {
      if (sex == "prop_female") {
        # standardise IVs
        var_std <- var %>% 
          mutate_if(is.numeric, scale, center=TRUE, scale=TRUE)
        
        print(psych::describe(var_std))
        
        fm <- as.formula(paste0("vars$", dv, "~", paste0(i, "*", sex, " + .")))
      } else if (sex %in% c("sex_driver", "sex_co_driver")) {
        # standardise IVs
        if (sex == "sex_driver") {
        var_std <- var %>% 
          mutate(sex_driver = factor(sex_driver)) %>%
          mutate_if(is.numeric, scale, center=TRUE, scale=TRUE)
        } else if (sex == "sex_co_driver") {
        var_std <- var %>% 
          mutate(sex_co_driver = factor(sex_co_driver)) %>%
          mutate_if(is.numeric, scale, center=TRUE, scale=TRUE)
        }
        
        print(psych::describe(var_std))
        
        fm <- as.formula(paste0("vars$", dv, "~", paste0(i, "*", sex, " + .")))
        
      }
      
      f <- lm(fm, data = var_std)
      
      summary(f)
    }
  })
}

```

## Correlations between all variables
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# select vars
# aus_born 1 == "Australia" 0 == "other"
# eng_fl 1 == "english" 0 == "other"

x <- vars %>% 
  select(sim_metrics, comms_fac, age_co_driver:sex_driver, prop_female,
         driving_years:neuroticism, driving_years_drone:neuroticism_drone, 
         contains("nasa"), -contains("aus_years"), -contains("dic_use"))



kable(star_matrix(x))

```

## Predicting the communication factors
## Which variables sig. correlate with the communication factors?
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# select variables and create correlation matrix
x <- vars %>% 
  select(comms_fac, age_co_driver:aus_born_driver, eng_fl_co_driver:prop_female, 
         driving_years:neuroticism, driving_years_drone:neuroticism_drone, 
         contains("nasa")) %>% 
  psych::corr.test() 

x <- sig_corrs(x, comms_fac)

kable(x)

```

<!-- ``` {r echo=FALSE, message=FALSE, warning=FALSE} -->
<!-- # conduct regression using a loop -->
<!-- map(comms_fac, function(i) { -->
<!--   cat("  \n###",  i, "  \n") -->
<!-- # set the dv -->
<!--   dv <- i -->

<!--   # select variables for the regression models -->
<!--     a <- bind_rows(x) %>%  -->
<!--       select(rowname, dv) %>%  -->
<!--       na.omit() %>% select(rowname) %>%  -->
<!--       filter(!rowname %in% c("sex_driver", "sex_co_driver", "sit.awareness_driver",  -->
<!--                              "sit.awareness_co_driver", "sit.awareness", -->
<!--                              "leadership_driver", "leadership_co_driver", "leadership")) -->

<!--     var <- vars %>% select(a$rowname) -->


<!--   # function to fit regression model, and run diagnostics for overall simulation-derived metrics -->
<!--     # print dv name -->
<!--     print(paste0("DV = ", dv)) -->
<!--     n <- names(var) -->

<!--     # var_std <- var %>%  -->
<!--     #   # mutate(prop_female = factor(prop_female)) %>%  -->
<!--     #   mutate_if(is.numeric, scale, center=TRUE, scale=TRUE) -->
<!--     # print(psych::describe(var_std)) -->

<!--     # create regression formula -->
<!--     fm <- as.formula(paste0("vars$", dv, " ~ ", paste("scale(", n, ")", collapse = " + "))) -->

<!--     # fit model -->
<!--     f <- lm(fm, data = var) -->

<!--     # results -->
<!--     print(summary(f)) -->

<!--     # print zero-order, semi-part and partial correlations -->
<!--     data <- cbind(vars %>% select(dv), var) %>% na.omit() -->
<!--     library(ppcor) # package requires MASS which removes the select function from dplyr -->
<!--     r <- cor(data) -->
<!--     part <- pcor(data) -->
<!--     spart <- spcor(data) -->
<!--     cor <- data.frame(zero_order = round(r[1,c(2:length(data))],2), -->
<!--                       partial = round(part[[1]][1,c(2:length(data))],2), -->
<!--                       part = round(spart[[1]][1,c(2:length(data))],2)) -->

<!--     print(cor) -->

<!--     # detach packages and reload dplyr -->
<!--     detach("package:ppcor", unload=TRUE) -->
<!--     detach("package:MASS", unload=TRUE) -->
<!--     library(dplyr) -->

<!--     # check assumptions and multicollinearity -->
<!--     plot(f) -->

<!--     # plot correlations between IVs -->
<!--     p <- vars %>% select(n) %>%  -->
<!--       ggcorr(method = "pairwise.complete.obs",  -->
<!--              label = TRUE, label_size = 3, label_round = 2, label_alpha = TRUE, # add text correlations value to plot -->
<!--              hjust = 0.9, size = 4, color = "grey50", layout.exp = 3) # move variable labels -->

<!--     # more detailed plot: distribution, scatterplot, correlations -->
<!--     # vars %>% select(x, issue) %>%  -->
<!--     #   ggpairs(., lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1))) -->

<!--     # print correlation plot -->
<!--     print(p) -->

<!--     # check for multicollinearity -->
<!--     print(omcdiag(vars %>% select(n), vars %>% select(dv))) -->

<!--     print(imcdiag(vars %>% select(n), vars %>% select(dv))) -->

<!-- cat("  \n") -->

<!-- }) -->

<!-- ``` -->

## Inconsistent codriver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "inconsistent_codriver"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

```

## Helpful exchange
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "helpful_exchange"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv, comm = TRUE)

```

## Terrible codriver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "terrible_codriver"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv, comm = TRUE)

```

## Predicting the driving-simulation metrics
## Which variables sig. correlate with the driving-simulation metrics overall?
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# include nasa factors?
x <- vars %>% 
  select(sim_metrics, comms_fac, age_co_driver:aus_born_driver, eng_fl_co_driver:prop_female, driving_years:neuroticism, driving_years_drone:neuroticism_drone) %>% 
  psych::corr.test() 

x <- sig_corrs(x, sim_metrics)

kable(x)
```

## Collisions overall
## Using proportion of females
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "collisions_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

# fit regression with interactions bw comms components and prop_female
interact(dv)

# alternative method of checking assumptions and multicollinearity
# library("olsrr")
# Vignette: https://cran.r-project.org/web/packages/olsrr/vignettes/regression_diagnostics.html
# VIFs > 4 require investigation
# Condition index: Collinearity is spotted by finding 2 or more variables that have large proportions of variance (.50 or more) that correspond to large condition indices. A rule of thumb is to label as large those condition indices in the range of 30 or larger.
# ols_coll_diag(fit)
# ols_correlations(fit) # Relative importance of independent variables in determining Y. How much each variable uniquely contributes to R2 over and above that which can be accounted for by the other predictors.
# ols_plot_diagnostics(fit)
# ols_plot_resid_regressor(fit)
```

## Using sex of driver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "collisions_overall"

# select IVs
var <- select_vars(dv, sex = TRUE)

# fit regression model
regress(dv, sex = TRUE)

# fit regression with interactions bw comms components and sex_driver
interact(dv, sex = "sex_driver")

```

## Speed overall
## Using proportion of females
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "speed_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

# fit regression with interactions bw comms components and prop_female
interact(dv)

```

## Using sex of driver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "speed_overall"

# select IVs
var <- select_vars(dv, sex = TRUE)

# fit regression model
regress(dv, sex = TRUE)

# fit regression with interactions bw comms components and sex_driver
interact(dv, sex = "sex_driver")

```

## Distance overall
Neither proportion of females or sex of driver correlate with distance so they were not included.
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "distance_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

```


## Which variables sig. correlate with the driving-simulation metrics during fog-free periods?
``` {r echo=FALSE, message=FALSE, warning=FALSE}
sim_metrics_no_fog <- c("collisions_no_fog_overall", "speed_no_fog_overall", "distance_no_fog_overall")

x <- vars %>% 
  select(sim_metrics_no_fog, comms_fac, age_co_driver:aus_born_driver, eng_fl_co_driver:prop_female, driving_years:neuroticism, driving_years_drone:neuroticism_drone) %>% 
  psych::corr.test() 

x <- sig_corrs(x, sim_metrics_no_fog)

kable(x)
```

## Collisions fog-free periods
## Using proportion of females
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "collisions_no_fog_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

# fit regression with interactions bw comms components and prop_female
interact(dv)

```

## Using sex of the codriver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "collisions_no_fog_overall"

# select IVs
var <- select_vars(dv, sex = TRUE)

# fit regression model
regress(dv, sex = TRUE)

# fit regression with interactions bw comms components and sex_driver
interact(dv, sex = "sex_co_driver")

```


## Speed fog-free periods
## Using proportion of female only
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "speed_no_fog_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

```

## Using sex of the driver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "speed_no_fog_overall"

# select IVs
var <- select_vars(dv, sex = TRUE)

# fit regression model
regress(dv, sex = TRUE)

# fit regression with interactions bw comms components and sex_driver
interact(dv, sex = "sex_driver")

```

## Distance fog-free periods
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "distance_no_fog_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

```


## Which variables sig. correlate with the driving-simulation metrics during fog event probes?
``` {r echo=FALSE, message=FALSE, warning=FALSE}
sim_metrics_fog <- c("collisions_fog_overall", "speed_fog_overall", "distance_fog_overall")

x <- vars %>% 
  select(sim_metrics_fog, comms_fac, age_co_driver:aus_born_driver, eng_fl_co_driver:prop_female, driving_years:neuroticism, driving_years_drone:neuroticism_drone) %>% 
  psych::corr.test() 

x <- sig_corrs(x, sim_metrics_fog)

kable(x)
```

## Collisions fog event probes
## Using proportion of females
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "collisions_fog_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

```

## Using sex of the driver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "collisions_fog_overall"

# select IVs
var <- select_vars(dv, sex = TRUE)

# fit regression model
regress(dv, sex = TRUE)

# fit regression with interactions bw comms components and sex_driver
interact(dv, sex = "sex_driver")

```

## Speed fog event probes
## Using proportion of females
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "speed_fog_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv, comm = TRUE)

# fit regression with interactions bw comms components and prop_female
interact(dv)

```

## Using sex of the driver
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "speed_fog_overall"

# select IVs
var <- select_vars(dv, sex = TRUE)

# fit regression model
regress(dv, sex = TRUE)

# fit regression with interactions bw comms components and sex_driver
interact(dv, sex = "sex_driver")

```

## Distance fog event probes
``` {r echo=FALSE, message=FALSE, warning=FALSE}
# set the dv
dv <- "distance_fog_overall"

# select IVs
var <- select_vars(dv)

# fit regression model
regress(dv)

```
